<div style="display: flex; gap: 2rem; align-items: flex-start; flex-wrap: wrap;">
  <!-- Columna izquierda: Código y resultados -->
  <div style="flex: 1; min-width: 320px;">
    <label style="font-weight: bold;">Código Python (Q-learning):</label>
    <pre style="background: #23272e; color: #e3e3e3; padding: 1em; border-radius: 8px; font-size: 0.98em; font-family: 'Fira Mono', 'Consolas', monospace; overflow-x: auto; box-shadow: 0 2px 8px #0001;">
import numpy as np
import random

# Parámetros del entorno
n_states = 4  # 4 estados (0, 1, 2, 3)
actions = [{{'['}}0, 1{{']'}}]  # 0 = izquierda, 1 = derecha
rewards = [{{'['}}0, 0, 0, 1{{']'}}]  # recompensa solo al llegar al estado 3

# Matriz Q inicial
Q = np.zeros((n_states, len(actions)))

# Parámetros de aprendizaje
alpha = 0.1     # tasa de aprendizaje
gamma = 0.9     # factor de descuento
epsilon = 0.2   # exploración

# Función para elegir acción con ε-greedy
def choose_action(state):
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)
    else:
        return np.argmax(Q[state])

# Simulación de episodios
for episode in range(100):
    state = 0  # iniciar siempre desde el estado 0

    while state != 3:  # hasta llegar al estado terminal
        action = choose_action(state)

        # Transición de estado
        next_state = state + 1 if action == 1 else max(0, state - 1)

        # Actualización Q-learning
        Q[state, action] = Q[state, action] + alpha * (
            rewards[next_state] + gamma * np.max(Q[next_state]) - Q[state, action]
        )

        state = next_state

# Mostrar la matriz Q aprendida
print("Q-table aprendida:")
print(Q)
    </pre>
  </div>
  <!-- Columna derecha: Explicación -->
  <div style="flex: 1; min-width: 320px;">
    <label style="font-weight: bold;">¿Qué hace este código?</label>
    <div style="background: #fffde7; padding: 1.2em 1em; border-radius: 8px; font-size: 1.05em; color: #444; box-shadow: 0 2px 8px #0001;">
      Este código implementa Q-learning para que un agente aprenda a llegar al estado 3 en un entorno con 4 estados (0 a 3). En cada estado, puede moverse a la izquierda (acción 0) o a la derecha (acción 1). Solo obtener el estado 3 da recompensa (1), los demás 0. El agente elige acciones con una mezcla de exploración y explotación (ε-greedy). Luego actualiza la matriz Q, que guarda el valor esperado de cada acción en cada estado, usando la recompensa recibida y la mejor recompensa futura estimada. Esto se repite por 100 episodios empezando siempre en el estado 0. Al final, la matriz Q muestra qué acción es mejor en cada estado para maximizar la recompensa. Así el agente “aprende” a tomar decisiones óptimas en este entorno simple.
    </div>
  </div>
</div>