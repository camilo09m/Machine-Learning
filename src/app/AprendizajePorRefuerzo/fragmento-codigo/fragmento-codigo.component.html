<div style="display: flex; gap: 2rem; align-items: flex-start; flex-wrap: wrap;">
  <!-- Columna izquierda: Código y resultados -->
  <div style="flex: 1; min-width: 320px;">
    <label style="font-weight: bold;">Código Python (Q-learning):</label>
    <pre style="background: #23272e; color: #e3e3e3; padding: 1em; border-radius: 8px; font-size: 0.98em; font-family: 'Fira Mono', 'Consolas', monospace; overflow-x: auto; box-shadow: 0 2px 8px #0001;">
import numpy as np
import random

# Parámetros del entorno
n_states = 4  # 4 estados (0, 1, 2, 3)
actions = [{{'['}}0, 1{{']'}}]  # 0 = izquierda, 1 = derecha
rewards = [{{'['}}0, 0, 0, 1{{']'}}]  # recompensa solo al llegar al estado 3

# Matriz Q inicial
Q = np.zeros((n_states, len(actions)))

# Parámetros de aprendizaje
alpha = 0.1     # tasa de aprendizaje
gamma = 0.9     # factor de descuento
epsilon = 0.2   # exploración

# Función para elegir acción con ε-greedy
def choose_action(state):
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)
    else:
        return np.argmax(Q[state])

# Simulación de episodios
for episode in range(100):
    state = 0  # iniciar siempre desde el estado 0

    while state != 3:  # hasta llegar al estado terminal
        action = choose_action(state)

        # Transición de estado
        next_state = state + 1 if action == 1 else max(0, state - 1)

        # Actualización Q-learning
        Q[state, action] = Q[state, action] + alpha * (
            rewards[next_state] + gamma * np.max(Q[next_state]) - Q[state, action]
        )

        state = next_state

# Mostrar la matriz Q aprendida
print("Q-table aprendida:")
print(Q)
    </pre>
  </div>
  <!-- Columna derecha: Explicación -->
  <div style="flex: 1; min-width: 320px;">
    <label style="font-weight: bold;">¿Qué hace este código?</label>
    <div style="background: #fffde7; padding: 1.2em 1em; border-radius: 8px; font-size: 1.05em; color: #444; box-shadow: 0 2px 8px #0001;">
      Este código implementa <b>Q-learning</b>, un algoritmo de aprendizaje por refuerzo, en un entorno muy simple con 4 estados y 2 acciones posibles (izquierda o derecha).
      <ul>
        <li>El agente siempre empieza en el estado 0 y su objetivo es llegar al estado 3, donde recibe una recompensa.</li>
        <li>La matriz Q almacena el valor aprendido de tomar cada acción en cada estado.</li>
        <li>En cada episodio, el agente explora o explota usando la estrategia <b>ε-greedy</b> (a veces elige al azar, a veces elige la mejor acción conocida).</li>
        <li>La matriz Q se actualiza en cada paso usando la fórmula de Q-learning, que combina la recompensa inmediata y el valor futuro esperado.</li>
        <li>Al final, se imprime la Q-table aprendida, que indica cuál es la mejor acción en cada estado.</li>
      </ul>
      <b>¿Por qué es importante?</b><br>
      Q-learning es la base de muchos algoritmos modernos de inteligencia artificial que aprenden a tomar decisiones secuenciales, como robots, videojuegos y sistemas de recomendación.
    </div>
  </div>
</div>